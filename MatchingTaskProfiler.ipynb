{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_matching_task_profile_info()\n",
    "displaySaveResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datautils import*\n",
    "import os\n",
    "import os.path as path\n",
    "from learningutils import *\n",
    "from sklearn import tree\n",
    "import json\n",
    "from matching_task import *\n",
    "import time\n",
    "\n",
    "matching_tasks_summary = pd.DataFrame(columns=['Dataset', 'Domain', '#records_source', '#records_target', 'count_record_pairs', '#match', '#non-match',\n",
    "                                               'count_attr','#short_string_attr', '#long_string_attr', '#numeric_attr','avg_density_all'])\n",
    "matching_tasks_baseline_rf_results = pd.DataFrame(columns=['Dataset', 'precision','recall','f1','f1_std','proba_scores',\n",
    "                                                          'proba_scores_std','x-val f1','x-val f1 sigma'])\n",
    "matching_tasks_baseline_svm_results = pd.DataFrame(columns=['Dataset', 'precision','recall','f1','f1_std','proba_scores',\n",
    "                                                      'proba_scores_std','x-val f1','x-val f1 sigma'])\n",
    "\n",
    "matching_tasks_profiling = pd.DataFrame(columns=['Dataset','F1_xval_max', 'F1_xval_top_matching_relevant_features', \n",
    "                                                 'matching_relevant_features', \n",
    "   'matching_relevant_attributes','matching_relevant_attributes_density','matching_relevant_attributes_count',\n",
    " 'matching_relevant_attributes_datatypes','top_matching_relevant_features','top_relevant_attributes', \n",
    " 'top_relevant_attributes_count','top_relevant_attributes_datatypes', 'top_relevant_attributes_density',\n",
    "'avg_length_tokens_top_relevant_attributes','avg_length_words_top_relevant_attributes','corner_cases_top_matching_relevant_features'])\n",
    "                                                           \n",
    "# Use the flags below to indicate which results should be calculates\n",
    "summaryFeatures=True\n",
    "baselineResults=False\n",
    "profilingFeatures = True                                                           \n",
    "\n",
    "\n",
    "main_path = '/work/aprimpel/datasets_for_notebooks/datasetProfilingtrain_test_val/'\n",
    "dataset_dict_path='/work/aprimpel/datasets_for_notebooks/datasetProfilingtrain_test_val/leipzig_datasets/dataset_dict.txt' # contains metadata for the distinct tasks\n",
    "source_folder=\"leipzig_datasets\"\n",
    "\n",
    "#add the correct separators of the source sets and the gold standard\n",
    "sep_for_source_files= ','\n",
    "gs_sep = ','\n",
    "train_test_val=True # otherwise nested x-validation for baseline experiments\n",
    "\n",
    "#change for allowing multithreading\n",
    "threads=-1\n",
    "\n",
    "with open(dataset_dict_path) as json_file:\n",
    "    dict_ = json.load(json_file)\n",
    "\n",
    "def get_matching_task_profile_info():\n",
    "     \n",
    "    dat_counter = 0\n",
    "    for r, d, f in os.walk(main_path+source_folder+'/'):\n",
    "        print(\"Current path :\"+r)\n",
    "            \n",
    "        if r!=main_path+source_folder+'/':\n",
    "            dataset_name = r.split(\"/\")[-1]\n",
    "            dataset_ext_name =  dict_.get(dataset_name).get(\"ext_name\")\n",
    "            dataset_domain = dict_.get(dataset_name).get(\"domain\")\n",
    "            dataset_primAttr = dict_.get(dataset_name).get(\"primattr\")\n",
    "\n",
    "        else: continue\n",
    "        ds1=pd.DataFrame()\n",
    "        ds2=pd.DataFrame()\n",
    "        gs=pd.DataFrame()\n",
    "        for file in f:\n",
    "\n",
    "            feature_vector_train,feature_vector_test,feature_vector_validation = None, None,  None\n",
    "\n",
    "            if '.csv' in file and 'standard_2' in file:\n",
    "                gs = pd.read_csv(r+'/'+file, sep=gs_sep)\n",
    "\n",
    "            if path.exists(r+\"/feature_vector_train.csv\"):\n",
    "                feature_vector_train = pd.read_csv(r+\"/feature_vector_train.csv\", sep =\",\", engine='python')\n",
    "            if path.exists(r+\"/feature_vector_test.csv\"):\n",
    "                feature_vector_test = pd.read_csv(r+\"/feature_vector_test.csv\", sep =\",\", engine='python')\n",
    "            if path.exists(r+\"/feature_vector_validation.csv\"):\n",
    "                feature_vector_validation = pd.read_csv(r+\"/feature_vector_validation.csv\", sep =\",\", engine='python')\n",
    "            if '.csv' in file and 'standard' not in file and 'feature' not in file:\n",
    "                if '1_' in file : ds1= pd.read_csv(r+'/'+file, sep =sep_for_source_files, engine='python')\n",
    "                if '2_' in file:  ds2= pd.read_csv(r+'/'+file, sep =sep_for_source_files, engine='python')\n",
    "\n",
    "            if not ds1.empty and not ds2.empty and not gs.empty: break    \n",
    "\n",
    "\n",
    "        if not ds1.empty and not ds2.empty and not gs.empty:\n",
    "            ds1['subject_id'] = ds1['subject_id'].apply(str)\n",
    "            ds2['subject_id'] = ds2['subject_id'].apply(str)\n",
    "\n",
    "            if (source_folder==\"leipzig_datasets\" or source_folder==\"corleone_datasets\"):\n",
    "                ds1['subject_id'] = ds1['subject_id'].str.lower()\n",
    "                ds2['subject_id'] = ds2['subject_id'].str.lower()\n",
    "\n",
    "\n",
    "            gs['source_id'] = gs['source_id'].apply(str)\n",
    "            gs['target_id'] = gs['target_id'].apply(str)\n",
    "\n",
    "            if path.exists(r+\"/feature_vector.csv\"):\n",
    "                feature_vector = pd.read_csv(r+\"/feature_vector.csv\", sep =\",\", engine='python')\n",
    "            else: \n",
    "                feature_vector = createFeatureVectorFile(ds1,ds2,gs, embeddings = False, printProgress=True, \n",
    "                                                         saveFile=r+\"/feature_vector.csv\", threads=threads)\n",
    "\n",
    "\n",
    "            common_attributes = [value for value in ds1.columns if (value in ds2.columns and value!='subject_id')] \n",
    "\n",
    "            matching_task = MatchingTask(ds1, ds2, gs, feature_vector, dict_.get(dataset_name), common_attributes, feature_vector_train,feature_vector_test,feature_vector_validation)\n",
    "            if (summaryFeatures):\n",
    "                matching_task.getSummaryFeatures()\n",
    "                #correspondes features\n",
    "                summary_features = matching_task.dict_summary\n",
    "                summary_features['Dataset'] = dataset_ext_name\n",
    "                summary_features['Domain'] = dataset_domain\n",
    "                for key in matching_tasks_summary.columns:\n",
    "                    matching_tasks_summary.loc[dat_counter, key] = summary_features.get(key)\n",
    "\n",
    "\n",
    "            if (baselineResults):\n",
    "                # get baseline results\n",
    "                if (train_test_val): \n",
    "                    print(\"Evaluation with train_validation_test split\")\n",
    "                    matching_task.getSplitValidationResults(model=\"linear\")\n",
    "                    matching_task.getSplitValidationResults(model=\"non-linear\")\n",
    "                else:\n",
    "                    print(\"Evaluation with Nested-X-Validation (no splits will be considered) - slow for large tasks\")\n",
    "                    matching_task.getNestedXValidationResults(model=\"linear\")\n",
    "                    matching_task.getNestedXValidationResults(model=\"non-linear\")\n",
    "\n",
    "                #linear model results\n",
    "                linear_results = matching_task.dict_linear_results\n",
    "                linear_results['Dataset'] = dataset_ext_name\n",
    "                for key in linear_results:\n",
    "                    matching_tasks_baseline_svm_results.loc[dat_counter, key] = linear_results.get(key)\n",
    "                #non-linear model results\n",
    "                non_linear_results = matching_task.dict_non_linear_results\n",
    "                non_linear_results['Dataset'] = dataset_ext_name\n",
    "                for key in non_linear_results:\n",
    "                    matching_tasks_baseline_rf_results.loc[dat_counter, key] = non_linear_results.get(key)\n",
    "\n",
    "            if(profilingFeatures):\n",
    "                matching_task.getProfilingFeatures()\n",
    "                ident_features_profile =  matching_task.dict_profiling_features\n",
    "                ident_features_profile['Dataset'] = dataset_ext_name\n",
    "                for key in matching_tasks_profiling.columns:\n",
    "                    matching_tasks_profiling.loc[dat_counter,key] = str(ident_features_profile.get(key))                                    \n",
    "\n",
    "\n",
    "            dat_counter+=1\n",
    "    \n",
    "    \n",
    "def displaySaveResults(): \n",
    "    timestr = time.strftime(\"%m%d_%H%M%S\")\n",
    "    if (profilingFeatures):\n",
    "        display(matching_tasks_summary)\n",
    "        matching_tasks_summary.to_csv(main_path+source_folder+'/'+timestr+'matching_tasks_profiling.csv', index=False)\n",
    "\n",
    "    if (baselineResults):\n",
    "        display(matching_tasks_svm_results)\n",
    "        display(matching_tasks_rf_results)\n",
    "        matching_tasks_rf_results.to_csv(main_path+source_folder+'/'+timestr+'matching_tasks_RF_results.csv', index=False)\n",
    "        matching_tasks_svm_results.to_csv(main_path+source_folder+'/'+timestr+'matching_tasks_SVM_results.csv', index=False)\n",
    "\n",
    "    if(profilingFeatures):\n",
    "        display(matching_tasks_profiling)\n",
    "        matching_tasks_profiling.to_csv(main_path+source_folder+'/'+timestr+'matching_tasks_profiling_features_summary.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
